{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7_expirements.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLUnOyE4zMIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install ttictoc\n",
        "!pip3 uninstall -y tensorflow\n",
        "!pip3 install tensorflow==2.1.0\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BanxrHDYzQrF",
        "colab_type": "code",
        "outputId": "c0b4fb55-2a95-4667-8d8b-04b9e990722c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "import tensorflow.keras\n",
        "from tensorflow import keras\n",
        "\n",
        "BASE_PATH=\"\"\n",
        "NEGATIVE_DATASET_DIRECTORY = '/neg/'\n",
        "POSITIVE_DATASET_DIRECTORY = '/pos/'\n",
        "TRAIN_DATASET_DIRECTORY = BASE_PATH + 'train'\n",
        "TEST_DATASET_DIRECTORY = BASE_PATH + 'test'\n",
        "TRAIN_INDICES_DATASET_FILE_NAME = BASE_PATH + 'train_processed_indices.csv'\n",
        "TEST_INDICES_DATASET_FILE_NAME = BASE_PATH + 'test_processed_indices.csv'\n",
        "\n",
        "EPOCHS_NUMBER = 200\n",
        "\n",
        "TPU_BATCH_SIZE = 128*8\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "EMBEDDING_VECTOR_SIZE=300\n",
        "LSTM_UNITS_COUNT=512\n",
        "VOCABULARY_SIZE = 89527\n",
        "\n",
        "\n",
        "def read_dataset(dataset_directory):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    positive_reviews_directory = dataset_directory + POSITIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(positive_reviews_directory):\n",
        "        full_file_name = positive_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(1)\n",
        "    negative_reviews_directory = dataset_directory + NEGATIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(negative_reviews_directory):\n",
        "        full_file_name = negative_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(0)\n",
        "    return np.array(dataset), np.array(labels)\n",
        "\n",
        "\n",
        "def load_vocabulary():\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    with open('imdb.vocab', 'r') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary[line.replace('\\n', '')] = i\n",
        "            i += 1\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary(text, vocabulary):\n",
        "    found_words_codes = set()\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.add(vocabulary[word])\n",
        "    return found_words_codes\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary_preserving_order(text, vocabulary):\n",
        "    found_words_codes = []\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.append(vocabulary[word])\n",
        "    return pd.Series(found_words_codes).drop_duplicates().tolist()\n",
        "\n",
        "def to_feature_vector(found_words_codes, vocabulary):\n",
        "    x = np.array([0]*len(vocabulary))\n",
        "    for code in found_words_codes:\n",
        "        x[int(code) - 1] = 1\n",
        "    return x.reshape(1, -1)\n",
        "\n",
        "\n",
        "def dataset_to_features(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary(sample, vocabulary)\n",
        "        mapped_sample = to_feature_vector(found_words_codes, vocabulary)\n",
        "        mapped_dataset.append(mapped_sample)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def dataset_to_indices(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary_preserving_order(sample, vocabulary)\n",
        "        mapped_dataset.append(found_words_codes)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def save_indices_processed_as_csv(mapped_dataset, labels, output):\n",
        "    mapped_dataset_in_string = []\n",
        "    for sample in mapped_dataset:\n",
        "        mapped_dataset_in_string.append(' '.join(map(str, sample)) if sample else '')\n",
        "    dataset = np.column_stack([labels, np.array(mapped_dataset_in_string)])\n",
        "    np.savetxt(output, dataset, delimiter=',', fmt='%s')\n",
        "\n",
        "\n",
        "def load_indices_processed_as_csv(output):\n",
        "    csv = pd.read_csv(output)\n",
        "    labels = csv.iloc[:, 0].to_numpy()\n",
        "    dataset = []\n",
        "    for sample in csv.iloc[:, 1:].to_numpy():\n",
        "        dataset.append(list(map(np.float32, sample[0].split(' '))) if isinstance(sample[0], str) else [])\n",
        "    return np.array(dataset), labels\n",
        "\n",
        "\n",
        "def add_conv_layer(model, filters, dropout_rate):\n",
        "    model.add(\n",
        "        keras.layers.Conv1D(filters, 11, padding='same', kernel_initializer='he_normal', activation=\"relu\"))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.MaxPooling1D(padding='same'))\n",
        "    model.add(keras.layers.Dropout(dropout_rate))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train_dataset, train_labels = read_dataset(TRAIN_DATASET_DIRECTORY)\n",
        "    # test_dataset, test_labels = read_dataset(TEST_DATASET_DIRECTORY)\n",
        "    # print(train_dataset.shape)\n",
        "    # print(test_dataset.shape)\n",
        "\n",
        "    vocabulary = load_vocabulary()\n",
        "\n",
        "    # mapped_train_dataset = dataset_to_indices(train_dataset, vocabulary)\n",
        "    # print(mapped_train_dataset.shape)\n",
        "    # mapped_test_dataset = dataset_to_indices(test_dataset, vocabulary)\n",
        "    # print(mapped_test_dataset.shape)\n",
        "\n",
        "    # save_indices_processed_as_csv(mapped_train_dataset, train_labels, TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    # save_indices_processed_as_csv(mapped_test_dataset, test_labels, TEST_INDICES_DATASET_FILE_NAME)\n",
        "\n",
        "    mapped_train_dataset, train_labels = load_indices_processed_as_csv(TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_train_dataset]))\n",
        "    print(mapped_train_dataset.shape)\n",
        "    batch_remainder = mapped_train_dataset.shape[0] % (TPU_BATCH_SIZE)\n",
        "    mapped_train_dataset=mapped_train_dataset[:-batch_remainder]\n",
        "    train_labels=train_labels[:-batch_remainder]\n",
        "    mapped_test_dataset, test_labels = load_indices_processed_as_csv(TEST_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_test_dataset]))\n",
        "    print(mapped_test_dataset.shape)\n",
        "\n",
        "    print('Pad sequences (samples x time)')\n",
        "    x_train = sequence.pad_sequences(mapped_train_dataset, maxlen=MAX_LENGTH)\n",
        "    x_test = sequence.pad_sequences(mapped_test_dataset, maxlen=MAX_LENGTH)\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print('x_test shape:', x_test.shape)\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(BASE_PATH, 'glove.6B.300d.txt'))\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_VECTOR_SIZE))\n",
        "    asd = 0\n",
        "    for word, i in vocabulary.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            asd+=1\n",
        "    print(asd)\n",
        "\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "    with strategy.scope():\n",
        "        print('Build model...')\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(VOCABULARY_SIZE,\n",
        "                            EMBEDDING_VECTOR_SIZE,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_LENGTH,\n",
        "                            trainable=False))\n",
        "        model.add(Bidirectional(LSTM(LSTM_UNITS_COUNT, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)))\n",
        "        model.add(Bidirectional(LSTM(LSTM_UNITS_COUNT, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)))\n",
        "        # model.add(Conv1D(filters=128,\n",
        "        #        kernel_size=8,\n",
        "        #        strides=1,\n",
        "        #        activation='relu',\n",
        "        #        padding='same'))\n",
        "        add_conv_layer(model, 48, 0.2)\n",
        "        add_conv_layer(model, 64, 0.25)\n",
        "        add_conv_layer(model, 128, 0.3)\n",
        "        add_conv_layer(model, 160, 0.4)\n",
        "        add_conv_layer(model, 192, 0.5)\n",
        "        add_conv_layer(model, 192, 0.5)\n",
        "        add_conv_layer(model, 192, 0.5)\n",
        "        add_conv_layer(model, 192, 0.5)\n",
        "        model.add(Dense(1024, activation='relu', kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(1024, activation='relu', kernel_initializer='he_normal'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # try using different optimizers and different optimizer configs\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "        model.summary()\n",
        "\n",
        "        print('Train...')\n",
        "        model.fit(x_train, train_labels, batch_size=TPU_BATCH_SIZE, epochs=EPOCHS_NUMBER)\n",
        "        score, acc = model.evaluate(x_test, test_labels)\n",
        "        print('Test accuracy:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "535\n",
            "(24999,)\n",
            "678\n",
            "(24999,)\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (24576, 128)\n",
            "x_test shape: (24999, 128)\n",
            "62596\n",
            "WARNING:tensorflow:TPU system 10.14.202.202:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.14.202.202:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.14.202.202:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.14.202.202:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 128, 300)          26858100  \n",
            "_________________________________________________________________\n",
            "bidirectional_23 (Bidirectio (None, 128, 1024)         3330048   \n",
            "_________________________________________________________________\n",
            "bidirectional_24 (Bidirectio (None, 128, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "conv1d_63 (Conv1D)           (None, 128, 48)           540720    \n",
            "_________________________________________________________________\n",
            "batch_normalization_62 (Batc (None, 128, 48)           192       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_62 (MaxPooling (None, 64, 48)            0         \n",
            "_________________________________________________________________\n",
            "dropout_88 (Dropout)         (None, 64, 48)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_64 (Conv1D)           (None, 64, 64)            33856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 64, 64)            256       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_63 (MaxPooling (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "dropout_89 (Dropout)         (None, 32, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_65 (Conv1D)           (None, 32, 128)           90240     \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 32, 128)           512       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_64 (MaxPooling (None, 16, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_90 (Dropout)         (None, 16, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_66 (Conv1D)           (None, 16, 160)           225440    \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 16, 160)           640       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_65 (MaxPooling (None, 8, 160)            0         \n",
            "_________________________________________________________________\n",
            "dropout_91 (Dropout)         (None, 8, 160)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_67 (Conv1D)           (None, 8, 192)            338112    \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 8, 192)            768       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_66 (MaxPooling (None, 4, 192)            0         \n",
            "_________________________________________________________________\n",
            "dropout_92 (Dropout)         (None, 4, 192)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_68 (Conv1D)           (None, 4, 192)            405696    \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 4, 192)            768       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_67 (MaxPooling (None, 2, 192)            0         \n",
            "_________________________________________________________________\n",
            "dropout_93 (Dropout)         (None, 2, 192)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_69 (Conv1D)           (None, 2, 192)            405696    \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 2, 192)            768       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_68 (MaxPooling (None, 1, 192)            0         \n",
            "_________________________________________________________________\n",
            "dropout_94 (Dropout)         (None, 1, 192)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_70 (Conv1D)           (None, 1, 192)            405696    \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 1, 192)            768       \n",
            "_________________________________________________________________\n",
            "max_pooling1d_69 (MaxPooling (None, 1, 192)            0         \n",
            "_________________________________________________________________\n",
            "dropout_95 (Dropout)         (None, 1, 192)            0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 1, 1024)           197632    \n",
            "_________________________________________________________________\n",
            "dropout_96 (Dropout)         (None, 1, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 1, 1024)           1049600   \n",
            "_________________________________________________________________\n",
            "dropout_97 (Dropout)         (None, 1, 1024)           0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 1, 1)              1025      \n",
            "=================================================================\n",
            "Total params: 40,182,085\n",
            "Trainable params: 13,321,649\n",
            "Non-trainable params: 26,860,436\n",
            "_________________________________________________________________\n",
            "Train...\n",
            "Train on 24576 samples\n",
            "Epoch 1/200\n",
            "24576/24576 [==============================] - 39s 2ms/sample - loss: 1.8119 - accuracy: 0.5012\n",
            "Epoch 2/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.8851 - accuracy: 0.4969\n",
            "Epoch 3/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.7916 - accuracy: 0.5009\n",
            "Epoch 4/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.7517 - accuracy: 0.5017\n",
            "Epoch 5/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.7456 - accuracy: 0.4997\n",
            "Epoch 6/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.7433 - accuracy: 0.4991\n",
            "Epoch 7/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.7258 - accuracy: 0.4998\n",
            "Epoch 8/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.7175 - accuracy: 0.5000\n",
            "Epoch 9/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.7119 - accuracy: 0.5003\n",
            "Epoch 10/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.7080 - accuracy: 0.4973\n",
            "Epoch 11/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.7045 - accuracy: 0.5031\n",
            "Epoch 12/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.7038 - accuracy: 0.4980\n",
            "Epoch 13/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6980 - accuracy: 0.5090\n",
            "Epoch 14/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6982 - accuracy: 0.5023\n",
            "Epoch 15/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6973 - accuracy: 0.5064\n",
            "Epoch 16/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6983 - accuracy: 0.5044\n",
            "Epoch 17/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6963 - accuracy: 0.4984\n",
            "Epoch 18/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6951 - accuracy: 0.5048\n",
            "Epoch 19/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.6955 - accuracy: 0.5035\n",
            "Epoch 20/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6947 - accuracy: 0.5079\n",
            "Epoch 21/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6959 - accuracy: 0.5030\n",
            "Epoch 22/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6959 - accuracy: 0.4993\n",
            "Epoch 23/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6950 - accuracy: 0.5072\n",
            "Epoch 24/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6946 - accuracy: 0.5071\n",
            "Epoch 25/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6948 - accuracy: 0.5052\n",
            "Epoch 26/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6954 - accuracy: 0.5085\n",
            "Epoch 27/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6954 - accuracy: 0.5035\n",
            "Epoch 28/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6945 - accuracy: 0.5051\n",
            "Epoch 29/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.6948 - accuracy: 0.5020\n",
            "Epoch 30/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6934 - accuracy: 0.5114\n",
            "Epoch 31/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.6945 - accuracy: 0.5137\n",
            "Epoch 32/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6938 - accuracy: 0.5138\n",
            "Epoch 33/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.6941 - accuracy: 0.5083\n",
            "Epoch 34/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6929 - accuracy: 0.5177\n",
            "Epoch 35/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.6912 - accuracy: 0.5288\n",
            "Epoch 36/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.6911 - accuracy: 0.5317\n",
            "Epoch 37/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.6853 - accuracy: 0.5555\n",
            "Epoch 38/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6745 - accuracy: 0.5870\n",
            "Epoch 39/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6453 - accuracy: 0.6399\n",
            "Epoch 40/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.6102 - accuracy: 0.6775\n",
            "Epoch 41/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.5611 - accuracy: 0.7222\n",
            "Epoch 42/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.5078 - accuracy: 0.7596\n",
            "Epoch 43/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4836 - accuracy: 0.7788\n",
            "Epoch 44/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4663 - accuracy: 0.7855\n",
            "Epoch 45/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.4566 - accuracy: 0.7921\n",
            "Epoch 46/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.4459 - accuracy: 0.7965\n",
            "Epoch 47/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.4397 - accuracy: 0.8001\n",
            "Epoch 48/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4361 - accuracy: 0.8057\n",
            "Epoch 49/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4263 - accuracy: 0.8083\n",
            "Epoch 50/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.4203 - accuracy: 0.8122\n",
            "Epoch 51/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4210 - accuracy: 0.8150\n",
            "Epoch 52/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4153 - accuracy: 0.8158\n",
            "Epoch 53/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.4143 - accuracy: 0.8177\n",
            "Epoch 54/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.4100 - accuracy: 0.8176\n",
            "Epoch 55/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.4055 - accuracy: 0.8230\n",
            "Epoch 56/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.4001 - accuracy: 0.8221\n",
            "Epoch 57/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3928 - accuracy: 0.8291\n",
            "Epoch 58/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3947 - accuracy: 0.8249\n",
            "Epoch 59/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3907 - accuracy: 0.8278\n",
            "Epoch 60/200\n",
            "24576/24576 [==============================] - 19s 760us/sample - loss: 0.3889 - accuracy: 0.8279\n",
            "Epoch 61/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3878 - accuracy: 0.8305\n",
            "Epoch 62/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3894 - accuracy: 0.8306\n",
            "Epoch 63/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3846 - accuracy: 0.8309\n",
            "Epoch 64/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3784 - accuracy: 0.8350\n",
            "Epoch 65/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3780 - accuracy: 0.8353\n",
            "Epoch 66/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3800 - accuracy: 0.8324\n",
            "Epoch 67/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3725 - accuracy: 0.8391\n",
            "Epoch 68/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3714 - accuracy: 0.8401\n",
            "Epoch 69/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3656 - accuracy: 0.8392\n",
            "Epoch 70/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3638 - accuracy: 0.8445\n",
            "Epoch 71/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3682 - accuracy: 0.8405\n",
            "Epoch 72/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3583 - accuracy: 0.8447\n",
            "Epoch 73/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3575 - accuracy: 0.8444\n",
            "Epoch 74/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3556 - accuracy: 0.8450\n",
            "Epoch 75/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3559 - accuracy: 0.8450\n",
            "Epoch 76/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3505 - accuracy: 0.8475\n",
            "Epoch 77/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3532 - accuracy: 0.8484\n",
            "Epoch 78/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3486 - accuracy: 0.8519\n",
            "Epoch 79/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3439 - accuracy: 0.8525\n",
            "Epoch 80/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3427 - accuracy: 0.8527\n",
            "Epoch 81/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3451 - accuracy: 0.8548\n",
            "Epoch 82/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3391 - accuracy: 0.8564\n",
            "Epoch 83/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3328 - accuracy: 0.8590\n",
            "Epoch 84/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3358 - accuracy: 0.8571\n",
            "Epoch 85/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3334 - accuracy: 0.8569\n",
            "Epoch 86/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3315 - accuracy: 0.8579\n",
            "Epoch 87/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3274 - accuracy: 0.8609\n",
            "Epoch 88/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3286 - accuracy: 0.8630\n",
            "Epoch 89/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3241 - accuracy: 0.8604\n",
            "Epoch 90/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3283 - accuracy: 0.8617\n",
            "Epoch 91/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3192 - accuracy: 0.8643\n",
            "Epoch 92/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3145 - accuracy: 0.8656\n",
            "Epoch 93/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3184 - accuracy: 0.8650\n",
            "Epoch 94/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3151 - accuracy: 0.8660\n",
            "Epoch 95/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3116 - accuracy: 0.8680\n",
            "Epoch 96/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3111 - accuracy: 0.8709\n",
            "Epoch 97/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.3086 - accuracy: 0.8706\n",
            "Epoch 98/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.3035 - accuracy: 0.8709\n",
            "Epoch 99/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.3040 - accuracy: 0.8699\n",
            "Epoch 100/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2961 - accuracy: 0.8759\n",
            "Epoch 101/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2952 - accuracy: 0.8763\n",
            "Epoch 102/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2970 - accuracy: 0.8743\n",
            "Epoch 103/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2978 - accuracy: 0.8748\n",
            "Epoch 104/200\n",
            "24576/24576 [==============================] - 19s 764us/sample - loss: 0.2914 - accuracy: 0.8782\n",
            "Epoch 105/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2882 - accuracy: 0.8806\n",
            "Epoch 106/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2884 - accuracy: 0.8783\n",
            "Epoch 107/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2848 - accuracy: 0.8816\n",
            "Epoch 108/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2805 - accuracy: 0.8826\n",
            "Epoch 109/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2797 - accuracy: 0.8850\n",
            "Epoch 110/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2792 - accuracy: 0.8842\n",
            "Epoch 111/200\n",
            "24576/24576 [==============================] - 19s 764us/sample - loss: 0.2747 - accuracy: 0.8862\n",
            "Epoch 112/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2693 - accuracy: 0.8861\n",
            "Epoch 113/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2683 - accuracy: 0.8874\n",
            "Epoch 114/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2655 - accuracy: 0.8907\n",
            "Epoch 115/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2631 - accuracy: 0.8901\n",
            "Epoch 116/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2617 - accuracy: 0.8907\n",
            "Epoch 117/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2568 - accuracy: 0.8927\n",
            "Epoch 118/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2500 - accuracy: 0.8972\n",
            "Epoch 119/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2491 - accuracy: 0.8980\n",
            "Epoch 120/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2503 - accuracy: 0.8996\n",
            "Epoch 121/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2457 - accuracy: 0.8986\n",
            "Epoch 122/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2394 - accuracy: 0.9015\n",
            "Epoch 123/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2392 - accuracy: 0.9044\n",
            "Epoch 124/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2346 - accuracy: 0.9065\n",
            "Epoch 125/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2339 - accuracy: 0.9070\n",
            "Epoch 126/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2302 - accuracy: 0.9069\n",
            "Epoch 127/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2274 - accuracy: 0.9084\n",
            "Epoch 128/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2245 - accuracy: 0.9105\n",
            "Epoch 129/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2295 - accuracy: 0.9065\n",
            "Epoch 130/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2180 - accuracy: 0.9122\n",
            "Epoch 131/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2145 - accuracy: 0.9136\n",
            "Epoch 132/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2149 - accuracy: 0.9132\n",
            "Epoch 133/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.2103 - accuracy: 0.9143\n",
            "Epoch 134/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.2075 - accuracy: 0.9176\n",
            "Epoch 135/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.2091 - accuracy: 0.9174\n",
            "Epoch 136/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1969 - accuracy: 0.9200\n",
            "Epoch 137/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1935 - accuracy: 0.9223\n",
            "Epoch 138/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1903 - accuracy: 0.9226\n",
            "Epoch 139/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1921 - accuracy: 0.9250\n",
            "Epoch 140/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1869 - accuracy: 0.9267\n",
            "Epoch 141/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1820 - accuracy: 0.9295\n",
            "Epoch 142/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1825 - accuracy: 0.9274\n",
            "Epoch 143/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1800 - accuracy: 0.9286\n",
            "Epoch 144/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1742 - accuracy: 0.9297\n",
            "Epoch 145/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1764 - accuracy: 0.9302\n",
            "Epoch 146/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1657 - accuracy: 0.9317\n",
            "Epoch 147/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1634 - accuracy: 0.9354\n",
            "Epoch 148/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1666 - accuracy: 0.9346\n",
            "Epoch 149/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1594 - accuracy: 0.9368\n",
            "Epoch 150/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1609 - accuracy: 0.9365\n",
            "Epoch 151/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1600 - accuracy: 0.9376\n",
            "Epoch 152/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1478 - accuracy: 0.9419\n",
            "Epoch 153/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1539 - accuracy: 0.9394\n",
            "Epoch 154/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1426 - accuracy: 0.9447\n",
            "Epoch 155/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1456 - accuracy: 0.9441\n",
            "Epoch 156/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1387 - accuracy: 0.9449\n",
            "Epoch 157/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1405 - accuracy: 0.9440\n",
            "Epoch 158/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1434 - accuracy: 0.9454\n",
            "Epoch 159/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1420 - accuracy: 0.9465\n",
            "Epoch 160/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1325 - accuracy: 0.9497\n",
            "Epoch 161/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1298 - accuracy: 0.9513\n",
            "Epoch 162/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1363 - accuracy: 0.9482\n",
            "Epoch 163/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1284 - accuracy: 0.9507\n",
            "Epoch 164/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1266 - accuracy: 0.9513\n",
            "Epoch 165/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1266 - accuracy: 0.9513\n",
            "Epoch 166/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1207 - accuracy: 0.9555\n",
            "Epoch 167/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1214 - accuracy: 0.9530\n",
            "Epoch 168/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1205 - accuracy: 0.9519\n",
            "Epoch 169/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1159 - accuracy: 0.9550\n",
            "Epoch 170/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1156 - accuracy: 0.9565\n",
            "Epoch 171/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1118 - accuracy: 0.9574\n",
            "Epoch 172/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1059 - accuracy: 0.9603\n",
            "Epoch 173/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.1106 - accuracy: 0.9574\n",
            "Epoch 174/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1047 - accuracy: 0.9605\n",
            "Epoch 175/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.1036 - accuracy: 0.9606\n",
            "Epoch 176/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.1035 - accuracy: 0.9615\n",
            "Epoch 177/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0986 - accuracy: 0.9623\n",
            "Epoch 178/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0987 - accuracy: 0.9630\n",
            "Epoch 179/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.0981 - accuracy: 0.9628\n",
            "Epoch 180/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0946 - accuracy: 0.9655\n",
            "Epoch 181/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.0952 - accuracy: 0.9640\n",
            "Epoch 182/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0999 - accuracy: 0.9627\n",
            "Epoch 183/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0917 - accuracy: 0.9666\n",
            "Epoch 184/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0921 - accuracy: 0.9657\n",
            "Epoch 185/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0868 - accuracy: 0.9680\n",
            "Epoch 186/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.0895 - accuracy: 0.9659\n",
            "Epoch 187/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0852 - accuracy: 0.9685\n",
            "Epoch 188/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0868 - accuracy: 0.9685\n",
            "Epoch 189/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0844 - accuracy: 0.9686\n",
            "Epoch 190/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0823 - accuracy: 0.9700\n",
            "Epoch 191/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.0798 - accuracy: 0.9705\n",
            "Epoch 192/200\n",
            "24576/24576 [==============================] - 19s 763us/sample - loss: 0.0859 - accuracy: 0.9664\n",
            "Epoch 193/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0811 - accuracy: 0.9689\n",
            "Epoch 194/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0763 - accuracy: 0.9716\n",
            "Epoch 195/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0806 - accuracy: 0.9714\n",
            "Epoch 196/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0795 - accuracy: 0.9705\n",
            "Epoch 197/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0762 - accuracy: 0.9709\n",
            "Epoch 198/200\n",
            "24576/24576 [==============================] - 19s 762us/sample - loss: 0.0717 - accuracy: 0.9737\n",
            "Epoch 199/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0715 - accuracy: 0.9747\n",
            "Epoch 200/200\n",
            "24576/24576 [==============================] - 19s 761us/sample - loss: 0.0698 - accuracy: 0.9748\n",
            "24999/24999 [==============================] - 37s 1ms/sample - loss: 0.5095 - accuracy: 0.8628\n",
            "Test accuracy: 0.8627545\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}