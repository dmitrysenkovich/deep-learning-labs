{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNbdOiPZrPpF",
        "colab_type": "code",
        "outputId": "5762087a-6ef9-4e78-ea40-46a9be0516a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# !pip3 install ttictoc\n",
        "# !pip3 uninstall -y tensorflow\n",
        "# !pip3 install tensorflow==2.1.0\n",
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# print(tf.__version__)\n",
        "# !wget http://wikipedia2vec.s3.amazonaws.com/models/en/2018-04-20/enwiki_20180420_300d.txt.bz2\n",
        "# !bzip2 -dk enwiki_20180420_300d.txt.bz2\n",
        "# !head enwiki_20180420_300d.txt\n",
        "!wc -l enwiki_20180420_300d.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4530031 enwiki_20180420_300d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfidGTJ_rmA8",
        "colab_type": "code",
        "outputId": "fc4fc825-6544-4ab4-8b91-b6e2e02fc208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "import tensorflow.keras\n",
        "\n",
        "BASE_PATH=\"\"\n",
        "NEGATIVE_DATASET_DIRECTORY = '/neg/'\n",
        "POSITIVE_DATASET_DIRECTORY = '/pos/'\n",
        "TRAIN_DATASET_DIRECTORY = BASE_PATH + 'train'\n",
        "TEST_DATASET_DIRECTORY = BASE_PATH + 'test'\n",
        "TRAIN_INDICES_DATASET_FILE_NAME = BASE_PATH + 'train_processed_indices.csv'\n",
        "TEST_INDICES_DATASET_FILE_NAME = BASE_PATH + 'test_processed_indices.csv'\n",
        "\n",
        "EPOCHS_NUMBER = 100\n",
        "\n",
        "TPU_BATCH_SIZE = 128*8\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "EMBEDDING_VECTOR_SIZE=300\n",
        "LSTM_UNITS_COUNT=128\n",
        "VOCABULARY_SIZE = 89527\n",
        "\n",
        "\n",
        "def read_dataset(dataset_directory):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    positive_reviews_directory = dataset_directory + POSITIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(positive_reviews_directory):\n",
        "        full_file_name = positive_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(1)\n",
        "    negative_reviews_directory = dataset_directory + NEGATIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(negative_reviews_directory):\n",
        "        full_file_name = negative_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(0)\n",
        "    return np.array(dataset), np.array(labels)\n",
        "\n",
        "\n",
        "def load_vocabulary():\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    with open('imdb.vocab', 'r') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary[line.replace('\\n', '')] = i\n",
        "            i += 1\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary(text, vocabulary):\n",
        "    found_words_codes = set()\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.add(vocabulary[word])\n",
        "    return found_words_codes\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary_preserving_order(text, vocabulary):\n",
        "    found_words_codes = []\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.append(vocabulary[word])\n",
        "    return pd.Series(found_words_codes).drop_duplicates().tolist()\n",
        "\n",
        "def to_feature_vector(found_words_codes, vocabulary):\n",
        "    x = np.array([0]*len(vocabulary))\n",
        "    for code in found_words_codes:\n",
        "        x[int(code) - 1] = 1\n",
        "    return x.reshape(1, -1)\n",
        "\n",
        "\n",
        "def dataset_to_features(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary(sample, vocabulary)\n",
        "        mapped_sample = to_feature_vector(found_words_codes, vocabulary)\n",
        "        mapped_dataset.append(mapped_sample)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def dataset_to_indices(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary_preserving_order(sample, vocabulary)\n",
        "        mapped_dataset.append(found_words_codes)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def save_indices_processed_as_csv(mapped_dataset, labels, output):\n",
        "    mapped_dataset_in_string = []\n",
        "    for sample in mapped_dataset:\n",
        "        mapped_dataset_in_string.append(' '.join(map(str, sample)) if sample else '')\n",
        "    dataset = np.column_stack([labels, np.array(mapped_dataset_in_string)])\n",
        "    np.savetxt(output, dataset, delimiter=',', fmt='%s')\n",
        "\n",
        "\n",
        "def load_indices_processed_as_csv(output):\n",
        "    csv = pd.read_csv(output)\n",
        "    labels = csv.iloc[:, 0].to_numpy()\n",
        "    dataset = []\n",
        "    for sample in csv.iloc[:, 1:].to_numpy():\n",
        "        dataset.append(list(map(np.float32, sample[0].split(' '))) if isinstance(sample[0], str) else [])\n",
        "    return np.array(dataset), labels\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train_dataset, train_labels = read_dataset(TRAIN_DATASET_DIRECTORY)\n",
        "    # test_dataset, test_labels = read_dataset(TEST_DATASET_DIRECTORY)\n",
        "    # print(train_dataset.shape)\n",
        "    # print(test_dataset.shape)\n",
        "\n",
        "    vocabulary = load_vocabulary()\n",
        "\n",
        "    # mapped_train_dataset = dataset_to_indices(train_dataset, vocabulary)\n",
        "    # print(mapped_train_dataset.shape)\n",
        "    # mapped_test_dataset = dataset_to_indices(test_dataset, vocabulary)\n",
        "    # print(mapped_test_dataset.shape)\n",
        "\n",
        "    # save_indices_processed_as_csv(mapped_train_dataset, train_labels, TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    # save_indices_processed_as_csv(mapped_test_dataset, test_labels, TEST_INDICES_DATASET_FILE_NAME)\n",
        "\n",
        "    mapped_train_dataset, train_labels = load_indices_processed_as_csv(TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_train_dataset]))\n",
        "    print(mapped_train_dataset.shape)\n",
        "    batch_remainder = mapped_train_dataset.shape[0] % (TPU_BATCH_SIZE)\n",
        "    mapped_train_dataset=mapped_train_dataset[:-batch_remainder]\n",
        "    train_labels=train_labels[:-batch_remainder]\n",
        "    mapped_test_dataset, test_labels = load_indices_processed_as_csv(TEST_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_test_dataset]))\n",
        "    print(mapped_test_dataset.shape)\n",
        "\n",
        "    print('Pad sequences (samples x time)')\n",
        "    x_train = sequence.pad_sequences(mapped_train_dataset, maxlen=MAX_LENGTH)\n",
        "    x_test = sequence.pad_sequences(mapped_test_dataset, maxlen=MAX_LENGTH)\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print('x_test shape:', x_test.shape)\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(BASE_PATH, 'enwiki_20180420_300d.txt'))\n",
        "    next(f)\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        split_index = len(values) - 300\n",
        "        word = ' '.join(values[:split_index])\n",
        "        coefs = np.asarray(values[split_index:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_VECTOR_SIZE))\n",
        "    asd = 0\n",
        "    for word, i in vocabulary.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            asd+=1\n",
        "    print(asd)\n",
        "\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "    with strategy.scope():\n",
        "        print('Build model...')\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(VOCABULARY_SIZE,\n",
        "                            EMBEDDING_VECTOR_SIZE,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_LENGTH,\n",
        "                            trainable=False))\n",
        "        model.add(Bidirectional(LSTM(LSTM_UNITS_COUNT, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # try using different optimizers and different optimizer configs\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        print('Train...')\n",
        "        model.fit(x_train, train_labels, batch_size=TPU_BATCH_SIZE, epochs=EPOCHS_NUMBER)\n",
        "        score, acc = model.evaluate(x_test, test_labels)\n",
        "        print('Test accuracy:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "535\n",
            "(24999,)\n",
            "678\n",
            "(24999,)\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (24576, 128)\n",
            "x_test shape: (24999, 128)\n",
            "65390\n",
            "INFO:tensorflow:Initializing the TPU system: 10.95.244.50:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.95.244.50:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 24576 samples\n",
            "Epoch 1/100\n",
            "24576/24576 [==============================] - 7s 265us/sample - loss: 0.6785 - accuracy: 0.5645\n",
            "Epoch 2/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.5821 - accuracy: 0.6971\n",
            "Epoch 3/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.5424 - accuracy: 0.7312\n",
            "Epoch 4/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.5193 - accuracy: 0.7496\n",
            "Epoch 5/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4959 - accuracy: 0.7653\n",
            "Epoch 6/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4868 - accuracy: 0.7686\n",
            "Epoch 7/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4714 - accuracy: 0.7776\n",
            "Epoch 8/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4647 - accuracy: 0.7817\n",
            "Epoch 9/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4525 - accuracy: 0.7901\n",
            "Epoch 10/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4343 - accuracy: 0.8016\n",
            "Epoch 11/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4270 - accuracy: 0.8018\n",
            "Epoch 12/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.4109 - accuracy: 0.8164\n",
            "Epoch 13/100\n",
            "24576/24576 [==============================] - 1s 32us/sample - loss: 0.3994 - accuracy: 0.8176\n",
            "Epoch 14/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.3949 - accuracy: 0.8237\n",
            "Epoch 15/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3933 - accuracy: 0.8213\n",
            "Epoch 16/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3782 - accuracy: 0.8330\n",
            "Epoch 17/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3762 - accuracy: 0.8311\n",
            "Epoch 18/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3643 - accuracy: 0.8379\n",
            "Epoch 19/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3546 - accuracy: 0.8439\n",
            "Epoch 20/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3513 - accuracy: 0.8470\n",
            "Epoch 21/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3459 - accuracy: 0.8485\n",
            "Epoch 22/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3382 - accuracy: 0.8522\n",
            "Epoch 23/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3387 - accuracy: 0.8532\n",
            "Epoch 24/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3357 - accuracy: 0.8514\n",
            "Epoch 25/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3253 - accuracy: 0.8593\n",
            "Epoch 26/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3207 - accuracy: 0.8619\n",
            "Epoch 27/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.3119 - accuracy: 0.8645\n",
            "Epoch 28/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3063 - accuracy: 0.8680\n",
            "Epoch 29/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3004 - accuracy: 0.8718\n",
            "Epoch 30/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.3019 - accuracy: 0.8701\n",
            "Epoch 31/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2951 - accuracy: 0.8743\n",
            "Epoch 32/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2888 - accuracy: 0.8760\n",
            "Epoch 33/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2897 - accuracy: 0.8758\n",
            "Epoch 34/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2780 - accuracy: 0.8831\n",
            "Epoch 35/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2715 - accuracy: 0.8867\n",
            "Epoch 36/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2701 - accuracy: 0.8879\n",
            "Epoch 37/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2650 - accuracy: 0.8885\n",
            "Epoch 38/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2598 - accuracy: 0.8910\n",
            "Epoch 39/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2513 - accuracy: 0.8953\n",
            "Epoch 40/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2460 - accuracy: 0.8969\n",
            "Epoch 41/100\n",
            "24576/24576 [==============================] - 1s 32us/sample - loss: 0.2487 - accuracy: 0.8970\n",
            "Epoch 42/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2416 - accuracy: 0.8997\n",
            "Epoch 43/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2385 - accuracy: 0.9013\n",
            "Epoch 44/100\n",
            "24576/24576 [==============================] - 1s 43us/sample - loss: 0.2350 - accuracy: 0.9014\n",
            "Epoch 45/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2345 - accuracy: 0.9036\n",
            "Epoch 46/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2268 - accuracy: 0.9059\n",
            "Epoch 47/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2187 - accuracy: 0.9123\n",
            "Epoch 48/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.2185 - accuracy: 0.9094\n",
            "Epoch 49/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2124 - accuracy: 0.9134\n",
            "Epoch 50/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.2062 - accuracy: 0.9167\n",
            "Epoch 51/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.2024 - accuracy: 0.9165\n",
            "Epoch 52/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1965 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1958 - accuracy: 0.9197\n",
            "Epoch 54/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1906 - accuracy: 0.9213\n",
            "Epoch 55/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1916 - accuracy: 0.9222\n",
            "Epoch 56/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1865 - accuracy: 0.9247\n",
            "Epoch 57/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1826 - accuracy: 0.9270\n",
            "Epoch 58/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1785 - accuracy: 0.9269\n",
            "Epoch 59/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1763 - accuracy: 0.9295\n",
            "Epoch 60/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1719 - accuracy: 0.9309\n",
            "Epoch 61/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1692 - accuracy: 0.9321\n",
            "Epoch 62/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1646 - accuracy: 0.9336\n",
            "Epoch 63/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1598 - accuracy: 0.9352\n",
            "Epoch 64/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1569 - accuracy: 0.9355\n",
            "Epoch 65/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1533 - accuracy: 0.9375\n",
            "Epoch 66/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1539 - accuracy: 0.9393\n",
            "Epoch 67/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1465 - accuracy: 0.9419\n",
            "Epoch 68/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1491 - accuracy: 0.9398\n",
            "Epoch 69/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1506 - accuracy: 0.9381\n",
            "Epoch 70/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1399 - accuracy: 0.9440\n",
            "Epoch 71/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1388 - accuracy: 0.9443\n",
            "Epoch 72/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1395 - accuracy: 0.9428\n",
            "Epoch 73/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1322 - accuracy: 0.9471\n",
            "Epoch 74/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1316 - accuracy: 0.9468\n",
            "Epoch 75/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1232 - accuracy: 0.9510\n",
            "Epoch 76/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.1197 - accuracy: 0.9540\n",
            "Epoch 77/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1234 - accuracy: 0.9502\n",
            "Epoch 78/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1196 - accuracy: 0.9533\n",
            "Epoch 79/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1153 - accuracy: 0.9555\n",
            "Epoch 80/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1100 - accuracy: 0.9574\n",
            "Epoch 81/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1092 - accuracy: 0.9576\n",
            "Epoch 82/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1059 - accuracy: 0.9589\n",
            "Epoch 83/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1020 - accuracy: 0.9599\n",
            "Epoch 84/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.1023 - accuracy: 0.9591\n",
            "Epoch 85/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0971 - accuracy: 0.9620\n",
            "Epoch 86/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.0973 - accuracy: 0.9627\n",
            "Epoch 87/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.0969 - accuracy: 0.9624\n",
            "Epoch 88/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0912 - accuracy: 0.9651\n",
            "Epoch 89/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0931 - accuracy: 0.9627\n",
            "Epoch 90/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0880 - accuracy: 0.9663\n",
            "Epoch 91/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.0880 - accuracy: 0.9670\n",
            "Epoch 92/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0850 - accuracy: 0.9682\n",
            "Epoch 93/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.0803 - accuracy: 0.9696\n",
            "Epoch 94/100\n",
            "24576/24576 [==============================] - 1s 34us/sample - loss: 0.0853 - accuracy: 0.9676\n",
            "Epoch 95/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0821 - accuracy: 0.9681\n",
            "Epoch 96/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0884 - accuracy: 0.9653\n",
            "Epoch 97/100\n",
            "24576/24576 [==============================] - 1s 33us/sample - loss: 0.0787 - accuracy: 0.9703\n",
            "Epoch 98/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.0763 - accuracy: 0.9715\n",
            "Epoch 99/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.0776 - accuracy: 0.9705\n",
            "Epoch 100/100\n",
            "24576/24576 [==============================] - 1s 35us/sample - loss: 0.0704 - accuracy: 0.9738\n",
            "24999/24999 [==============================] - 15s 593us/sample - loss: 0.5563 - accuracy: 0.8524\n",
            "Test accuracy: 0.8523941\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}