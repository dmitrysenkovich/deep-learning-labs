{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab7_glove.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbLakQh9evnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install ttictoc\n",
        "# !pip3 uninstall -y tensorflow\n",
        "# !pip3 install tensorflow==2.1.0\n",
        "# %tensorflow_version 2.x\n",
        "# import tensorflow as tf\n",
        "# print(tf.__version__)\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBzxlrzuey93",
        "colab_type": "code",
        "outputId": "564530a4-c730-44c3-f77a-c70ccd186858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import os.path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional\n",
        "import tensorflow.keras\n",
        "\n",
        "BASE_PATH=\"\"\n",
        "NEGATIVE_DATASET_DIRECTORY = '/neg/'\n",
        "POSITIVE_DATASET_DIRECTORY = '/pos/'\n",
        "TRAIN_DATASET_DIRECTORY = BASE_PATH + 'train'\n",
        "TEST_DATASET_DIRECTORY = BASE_PATH + 'test'\n",
        "TRAIN_INDICES_DATASET_FILE_NAME = BASE_PATH + 'train_processed_indices.csv'\n",
        "TEST_INDICES_DATASET_FILE_NAME = BASE_PATH + 'test_processed_indices.csv'\n",
        "\n",
        "EPOCHS_NUMBER = 100\n",
        "\n",
        "TPU_BATCH_SIZE = 128*8\n",
        "\n",
        "MAX_LENGTH = 128\n",
        "EMBEDDING_VECTOR_SIZE=300\n",
        "LSTM_UNITS_COUNT=128\n",
        "VOCABULARY_SIZE = 89527\n",
        "\n",
        "\n",
        "def read_dataset(dataset_directory):\n",
        "    dataset = []\n",
        "    labels = []\n",
        "    positive_reviews_directory = dataset_directory + POSITIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(positive_reviews_directory):\n",
        "        full_file_name = positive_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(1)\n",
        "    negative_reviews_directory = dataset_directory + NEGATIVE_DATASET_DIRECTORY\n",
        "    for review_file_name in os.listdir(negative_reviews_directory):\n",
        "        full_file_name = negative_reviews_directory + review_file_name\n",
        "        dataset.append(open(full_file_name, 'r').read())\n",
        "        labels.append(0)\n",
        "    return np.array(dataset), np.array(labels)\n",
        "\n",
        "\n",
        "def load_vocabulary():\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    with open('imdb.vocab', 'r') as vocabulary_file:\n",
        "        for line in vocabulary_file:\n",
        "            vocabulary[line.replace('\\n', '')] = i\n",
        "            i += 1\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary(text, vocabulary):\n",
        "    found_words_codes = set()\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.add(vocabulary[word])\n",
        "    return found_words_codes\n",
        "\n",
        "\n",
        "def find_words_from_vocabulary_preserving_order(text, vocabulary):\n",
        "    found_words_codes = []\n",
        "    for word in text.split(' '):\n",
        "        if word in vocabulary:\n",
        "            found_words_codes.append(vocabulary[word])\n",
        "    return pd.Series(found_words_codes).drop_duplicates().tolist()\n",
        "\n",
        "def to_feature_vector(found_words_codes, vocabulary):\n",
        "    x = np.array([0]*len(vocabulary))\n",
        "    for code in found_words_codes:\n",
        "        x[int(code) - 1] = 1\n",
        "    return x.reshape(1, -1)\n",
        "\n",
        "\n",
        "def dataset_to_features(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary(sample, vocabulary)\n",
        "        mapped_sample = to_feature_vector(found_words_codes, vocabulary)\n",
        "        mapped_dataset.append(mapped_sample)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def dataset_to_indices(dataset, vocabulary):\n",
        "    mapped_dataset = []\n",
        "    for sample in dataset:\n",
        "        found_words_codes = find_words_from_vocabulary_preserving_order(sample, vocabulary)\n",
        "        mapped_dataset.append(found_words_codes)\n",
        "    return np.array(mapped_dataset)\n",
        "\n",
        "\n",
        "def save_indices_processed_as_csv(mapped_dataset, labels, output):\n",
        "    mapped_dataset_in_string = []\n",
        "    for sample in mapped_dataset:\n",
        "        mapped_dataset_in_string.append(' '.join(map(str, sample)) if sample else '')\n",
        "    dataset = np.column_stack([labels, np.array(mapped_dataset_in_string)])\n",
        "    np.savetxt(output, dataset, delimiter=',', fmt='%s')\n",
        "\n",
        "\n",
        "def load_indices_processed_as_csv(output):\n",
        "    csv = pd.read_csv(output)\n",
        "    labels = csv.iloc[:, 0].to_numpy()\n",
        "    dataset = []\n",
        "    for sample in csv.iloc[:, 1:].to_numpy():\n",
        "        dataset.append(list(map(np.float32, sample[0].split(' '))) if isinstance(sample[0], str) else [])\n",
        "    return np.array(dataset), labels\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # train_dataset, train_labels = read_dataset(TRAIN_DATASET_DIRECTORY)\n",
        "    # test_dataset, test_labels = read_dataset(TEST_DATASET_DIRECTORY)\n",
        "    # print(train_dataset.shape)\n",
        "    # print(test_dataset.shape)\n",
        "\n",
        "    vocabulary = load_vocabulary()\n",
        "\n",
        "    # mapped_train_dataset = dataset_to_indices(train_dataset, vocabulary)\n",
        "    # print(mapped_train_dataset.shape)\n",
        "    # mapped_test_dataset = dataset_to_indices(test_dataset, vocabulary)\n",
        "    # print(mapped_test_dataset.shape)\n",
        "\n",
        "    # save_indices_processed_as_csv(mapped_train_dataset, train_labels, TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    # save_indices_processed_as_csv(mapped_test_dataset, test_labels, TEST_INDICES_DATASET_FILE_NAME)\n",
        "\n",
        "    mapped_train_dataset, train_labels = load_indices_processed_as_csv(TRAIN_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_train_dataset]))\n",
        "    print(mapped_train_dataset.shape)\n",
        "    batch_remainder = mapped_train_dataset.shape[0] % (TPU_BATCH_SIZE)\n",
        "    mapped_train_dataset=mapped_train_dataset[:-batch_remainder]\n",
        "    train_labels=train_labels[:-batch_remainder]\n",
        "    mapped_test_dataset, test_labels = load_indices_processed_as_csv(TEST_INDICES_DATASET_FILE_NAME)\n",
        "    print(np.max([len(v) for v in mapped_test_dataset]))\n",
        "    print(mapped_test_dataset.shape)\n",
        "\n",
        "    print('Pad sequences (samples x time)')\n",
        "    x_train = sequence.pad_sequences(mapped_train_dataset, maxlen=MAX_LENGTH)\n",
        "    x_test = sequence.pad_sequences(mapped_test_dataset, maxlen=MAX_LENGTH)\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print('x_test shape:', x_test.shape)\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join(BASE_PATH, 'glove.6B.300d.txt'))\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCABULARY_SIZE, EMBEDDING_VECTOR_SIZE))\n",
        "    asd = 0\n",
        "    for word, i in vocabulary.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            asd+=1\n",
        "    print(asd)\n",
        "\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "    with strategy.scope():\n",
        "        print('Build model...')\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(VOCABULARY_SIZE,\n",
        "                            EMBEDDING_VECTOR_SIZE,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_LENGTH,\n",
        "                            trainable=False))\n",
        "        model.add(Bidirectional(LSTM(LSTM_UNITS_COUNT, dropout=0.2, recurrent_dropout=0.2)))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        # try using different optimizers and different optimizer configs\n",
        "        model.compile(loss='binary_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        print('Train...')\n",
        "        model.fit(x_train, train_labels, batch_size=TPU_BATCH_SIZE, epochs=EPOCHS_NUMBER)\n",
        "        score, acc = model.evaluate(x_test, test_labels)\n",
        "        print('Test accuracy:', acc)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "535\n",
            "(24999,)\n",
            "678\n",
            "(24999,)\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (24576, 128)\n",
            "x_test shape: (24999, 128)\n",
            "62596\n",
            "WARNING:tensorflow:TPU system 10.71.194.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.71.194.58:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.71.194.58:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.71.194.58:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Train...\n",
            "Train on 24576 samples\n",
            "Epoch 1/100\n",
            "24576/24576 [==============================] - 7s 277us/sample - loss: 0.6570 - accuracy: 0.6051\n",
            "Epoch 2/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.5483 - accuracy: 0.7245\n",
            "Epoch 3/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.4808 - accuracy: 0.7747\n",
            "Epoch 4/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.4622 - accuracy: 0.7859\n",
            "Epoch 5/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.4450 - accuracy: 0.7983\n",
            "Epoch 6/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.4314 - accuracy: 0.8046\n",
            "Epoch 7/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.4137 - accuracy: 0.8124\n",
            "Epoch 8/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.3984 - accuracy: 0.8209\n",
            "Epoch 9/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.3841 - accuracy: 0.8282\n",
            "Epoch 10/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.3744 - accuracy: 0.8333\n",
            "Epoch 11/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.3753 - accuracy: 0.8342\n",
            "Epoch 12/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.3664 - accuracy: 0.8379\n",
            "Epoch 13/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.3524 - accuracy: 0.8455\n",
            "Epoch 14/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.3369 - accuracy: 0.8525\n",
            "Epoch 15/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.3334 - accuracy: 0.8562\n",
            "Epoch 16/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.3180 - accuracy: 0.8635\n",
            "Epoch 17/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.3107 - accuracy: 0.8668\n",
            "Epoch 18/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.3048 - accuracy: 0.8696\n",
            "Epoch 19/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2901 - accuracy: 0.8763\n",
            "Epoch 20/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.2864 - accuracy: 0.8774\n",
            "Epoch 21/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.2720 - accuracy: 0.8864\n",
            "Epoch 22/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2616 - accuracy: 0.8894\n",
            "Epoch 23/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.2530 - accuracy: 0.8937\n",
            "Epoch 24/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.2491 - accuracy: 0.8978\n",
            "Epoch 25/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2311 - accuracy: 0.9043\n",
            "Epoch 26/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2244 - accuracy: 0.9093\n",
            "Epoch 27/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2101 - accuracy: 0.9154\n",
            "Epoch 28/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.2033 - accuracy: 0.9168\n",
            "Epoch 29/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.2017 - accuracy: 0.9181\n",
            "Epoch 30/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.1966 - accuracy: 0.9205\n",
            "Epoch 31/100\n",
            "24576/24576 [==============================] - 1s 53us/sample - loss: 0.1920 - accuracy: 0.9211\n",
            "Epoch 32/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1699 - accuracy: 0.9299\n",
            "Epoch 33/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.1653 - accuracy: 0.9341\n",
            "Epoch 34/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.1624 - accuracy: 0.9342\n",
            "Epoch 35/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.1450 - accuracy: 0.9446\n",
            "Epoch 36/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1388 - accuracy: 0.9459\n",
            "Epoch 37/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.1299 - accuracy: 0.9488\n",
            "Epoch 38/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1219 - accuracy: 0.9533\n",
            "Epoch 39/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1225 - accuracy: 0.9521\n",
            "Epoch 40/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1163 - accuracy: 0.9544\n",
            "Epoch 41/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.1040 - accuracy: 0.9598\n",
            "Epoch 42/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.1019 - accuracy: 0.9623\n",
            "Epoch 43/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0961 - accuracy: 0.9630\n",
            "Epoch 44/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0908 - accuracy: 0.9659\n",
            "Epoch 45/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0890 - accuracy: 0.9667\n",
            "Epoch 46/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0791 - accuracy: 0.9707\n",
            "Epoch 47/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0801 - accuracy: 0.9690\n",
            "Epoch 48/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0748 - accuracy: 0.9725\n",
            "Epoch 49/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0680 - accuracy: 0.9764\n",
            "Epoch 50/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0625 - accuracy: 0.9775\n",
            "Epoch 51/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0603 - accuracy: 0.9781\n",
            "Epoch 52/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0576 - accuracy: 0.9790\n",
            "Epoch 53/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0553 - accuracy: 0.9810\n",
            "Epoch 54/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0540 - accuracy: 0.9808\n",
            "Epoch 55/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0510 - accuracy: 0.9818\n",
            "Epoch 56/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0452 - accuracy: 0.9849\n",
            "Epoch 57/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0429 - accuracy: 0.9856\n",
            "Epoch 58/100\n",
            "24576/24576 [==============================] - 1s 48us/sample - loss: 0.0418 - accuracy: 0.9859\n",
            "Epoch 59/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0418 - accuracy: 0.9853\n",
            "Epoch 60/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0389 - accuracy: 0.9869\n",
            "Epoch 61/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0383 - accuracy: 0.9876\n",
            "Epoch 62/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0352 - accuracy: 0.9887\n",
            "Epoch 63/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0342 - accuracy: 0.9891\n",
            "Epoch 64/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0305 - accuracy: 0.9906\n",
            "Epoch 65/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0307 - accuracy: 0.9899\n",
            "Epoch 66/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0299 - accuracy: 0.9901\n",
            "Epoch 67/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0291 - accuracy: 0.9905\n",
            "Epoch 68/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0275 - accuracy: 0.9916\n",
            "Epoch 69/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0281 - accuracy: 0.9906\n",
            "Epoch 70/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0271 - accuracy: 0.9915\n",
            "Epoch 71/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0258 - accuracy: 0.9925\n",
            "Epoch 72/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0232 - accuracy: 0.9927\n",
            "Epoch 73/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0230 - accuracy: 0.9922\n",
            "Epoch 74/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0236 - accuracy: 0.9924\n",
            "Epoch 75/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0218 - accuracy: 0.9937\n",
            "Epoch 76/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0241 - accuracy: 0.9927\n",
            "Epoch 77/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0205 - accuracy: 0.9938\n",
            "Epoch 78/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0205 - accuracy: 0.9932\n",
            "Epoch 79/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0197 - accuracy: 0.9935\n",
            "Epoch 80/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0177 - accuracy: 0.9945\n",
            "Epoch 81/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0197 - accuracy: 0.9936\n",
            "Epoch 82/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0202 - accuracy: 0.9937\n",
            "Epoch 83/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0176 - accuracy: 0.9946\n",
            "Epoch 84/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0173 - accuracy: 0.9947\n",
            "Epoch 85/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0183 - accuracy: 0.9943\n",
            "Epoch 86/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0164 - accuracy: 0.9952\n",
            "Epoch 87/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0200 - accuracy: 0.9939\n",
            "Epoch 88/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0158 - accuracy: 0.9958\n",
            "Epoch 89/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0175 - accuracy: 0.9947\n",
            "Epoch 90/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0166 - accuracy: 0.9944\n",
            "Epoch 91/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0144 - accuracy: 0.9958\n",
            "Epoch 92/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0161 - accuracy: 0.9952\n",
            "Epoch 93/100\n",
            "24576/24576 [==============================] - 1s 51us/sample - loss: 0.0138 - accuracy: 0.9956\n",
            "Epoch 94/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0128 - accuracy: 0.9960\n",
            "Epoch 95/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0124 - accuracy: 0.9963\n",
            "Epoch 96/100\n",
            "24576/24576 [==============================] - 1s 52us/sample - loss: 0.0119 - accuracy: 0.9966\n",
            "Epoch 97/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0127 - accuracy: 0.9959\n",
            "Epoch 98/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0118 - accuracy: 0.9965\n",
            "Epoch 99/100\n",
            "24576/24576 [==============================] - 1s 50us/sample - loss: 0.0144 - accuracy: 0.9954\n",
            "Epoch 100/100\n",
            "24576/24576 [==============================] - 1s 49us/sample - loss: 0.0150 - accuracy: 0.9954\n",
            "24999/24999 [==============================] - 17s 681us/sample - loss: 0.7421 - accuracy: 0.8532\n",
            "Test accuracy: 0.8531941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ecwEofe1Tf",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}